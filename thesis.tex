\documentclass{report}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb}
\usepackage{amsthm, thmtools}
\usepackage{etoolbox}  % for hooking into environments
\usepackage[shortlabels]{enumitem}
\usepackage[parboxrestore]{marginnote}
\usepackage{mathtools}
\usepackage{changepage}
\usepackage{cancel}
\usepackage{faktor}
\usepackage[protrusion=true,expansion=true]{microtype}	
\usepackage{float}
\usepackage[dvipsnames]{xcolor}
\usepackage{url}
\usepackage{nameref}
\usepackage{xspace}
\usepackage{dsfont} % need it for \mathbb{1}
\usepackage{dirtytalk}
\usepackage[parfill]{parskip} % do not indent new paragraphs
\usepackage[linktoc=all]{hyperref}
% \usepackage{cleveref}

\newcommand{\remarkautorefname}{Remark}

\hypersetup{
  linktoc=section,      % Only the number in the ToC is a link
  colorlinks=true,
  linkcolor=Blue,        % or any color you want
  citecolor=Green,
}

% \usepackage[a4paper, top=2.5cm, bottom=3cm, left=3cm, right=2.5cm, portrait]{geometry}
\usepackage{geometry}
\geometry{a4paper, portrait
%, margin=1.0in
}

% TEMPLATE STUFF
\usepackage{fancyhdr}

\usepackage{titlesec}
\titleformat{\chapter}{\bfseries\sffamily}{\huge\Roman{chapter}.}{10pt}{\huge}
\titlespacing*{\chapter}{0pt}{5.5ex plus 1ex minus .2ex}{1.3ex plus .2ex}
\titleclass{\chapter}{straight}

\usepackage[
    backend=biber,
    style=alphabetic,
    sorting=ynt
]{biblatex}
\addbibresource{references.bib}

\makeatletter
\renewcommand*\env@matrix[1][\arraystretch]{%
  \edef\arraystretch{#1}%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{*\c@MaxMatrixCols c}}
\makeatother

\newcommand{\FF}{\mathbb{F}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\NO}{{\mathbb{N}_0}}
\newcommand{\NP}{\mathbb{N}_+}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\KK}{\mathbb{K}}
\newcommand{\TT}{\mathbb{T}}

\DeclarePairedDelimiter\abs{\lvert}{\rvert} %NOTE: \abs* gives auto-sizing
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%
% \DeclarePairedDelimiter\norm\lVert\rVert
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

% \DeclareMathOperator{\Var}{Var}
\newcommand{\Var}[1]{\operatorname{Var}\left(#1\right)}
\newcommand{\Exp}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\CondExp}[2]{\mathbb{E}\left[#1 \mid #2\right]}
\newcommand{\DefEmph}[1]{\textbf{#1}} 
\newcommand{\Cov}[2]{\operatorname{Cov}\left(#1, #2\right)} 
\newcommand{\Corr}[2]{\operatorname{Corr}\left(#1, #2\right)}
\newcommand{\CharFunction}[1]{\mathds{1}_{#1}}
\newcommand{\Restrict}[2]{\left. #1 \right|_{#2}}
\newcommand{\ScalarProd}[2]{\left\langle #1, #2 \right\rangle}
\newcommand{\Placeholder}{\,\mathrel{\cdot}\,}
\DeclareMathOperator{\ProjOp}{\mathbf{P}}
\newcommand{\Convolve}[3][]{#2 \star_{#1} #3}

\DeclareMathOperator{\DConv}{\stackrel{(d)}{\longrightarrow}}
\DeclareMathOperator{\PConv}{\stackrel{\PP}{\longrightarrow}}
\DeclareMathOperator{\DEquiv}{\stackrel{(d)}{=}}
\DeclareMathOperator{\DefiningEquality}{\coloneqq}
\DeclareMathOperator{\DefiningEqualityRev}{\eqqcolon}
\DeclareMathOperator{\Union}{\cup}
\DeclareMathOperator{\DisUnion}{\dot{\cup}}
\DeclareMathOperator{\Intersection}{\cap}
\DeclareMathOperator{\Range}{ran}
\DeclareMathOperator{\Support}{supp}
\DeclareMathOperator{\Ext}{Ext}



% \DeclareMathOperator{\dist}{dist}
% \DeclareMathOperator{\weight}{weight}

\swapnumbers % optional, of course
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\renewenvironment{proof}{{\bf \emph{Proof:} }}{\hfill $\Box$ \\} 
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\AtBeginEnvironment{remark}{
  \pushQED{\qed}\renewcommand{\qedsymbol}{$\lrcorner$}%
}
\AtEndEnvironment{remark}{\popQED}


\AtBeginEnvironment{example}{%
  \pushQED{\qed}\renewcommand{\qedsymbol}{$\scriptstyle\triangle$}%
}
\AtEndEnvironment{example}{\popQED}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]


\newcommand{\thistheoremname}{}

\newtheorem{genericthm}[theorem]{\thistheoremname}
\newenvironment{namedthm}[1]
  {\renewcommand{\thistheoremname}{#1}%
   \begin{genericthm}}
  {\end{genericthm}}


\DeclareRobustCommand{\rchi}{{\mathpalette\irchi\relax}}
\newcommand{\irchi}[2]{\raisebox{\depth}{$#1\chi$}} % inner command, used by \rchi
\newcommand{\iid}{\text{i.i.d}\xspace}
\newcommand{\TODO}[1]{\text{\textcolor{red}{TODO: #1}}\xspace}

\let\oldphi\phi
\let\phi\varphi

\let\oldrho\rho
\let\rho\varrho

\let\oldepsilon\epsilon
\let\epsilon\varepsilon

\renewcommand{\chapterautorefname}{Chapter}
\renewcommand{\sectionautorefname}{Section}
\renewcommand{\subsectionautorefname}{Subsection}
\renewcommand{\subsubsectionautorefname}{Subsubsection}
\renewcommand{\figureautorefname}{Figure}
\renewcommand{\tableautorefname}{Table}
\renewcommand*{\equationautorefname}{\!} % effectively hides "Equation"
\newcommand{\fullref}[1]{\autoref{#1}~(\nameref{#1})}

\newcommand{\bigO}[1]{\ensuremath{\mathop{}\mathopen{}\mathcal{O}\mathopen{}\left(#1\right)}}

\newcommand\smallO[1]{
    \mathchoice
    {% mode \displaystyle
      \scriptstyle\mathcal{O}\left(#1\right)
    }
    {% mode \textstyle
      \scriptstyle\mathcal{O}\left(#1\right)
    }
    {% mode \scriptstyle
      \scriptscriptstyle\mathcal{O}\left(#1\right)
    }
    {% mode \scriptscriptstyle
      \scalebox{0.8}{$\scriptscriptstyle\mathcal{O}$}\left(#1\right)
    }
}


\title{\textsc{Convergence of the Two-Dimensional Dynamic Ising-Kac Model to $\Phi^4_2$}
\\[1.5em]
{\small {for}}\\[1.5em]
{\Large \textsc{Stochastic Partial Differential Equations: \\Classical and New}}\\
{\large (Dr. Guilherme de Lima Feltes)}}

\author{
    \textsc{BalÃ¡zs Kossovics}\\
    {\small (Matr. 5563286)}
}
\date{2025 Summer Semester}


\begin{document}

\maketitle

\newpage
\setcounter{page}{2}

\tableofcontents

\chapter{Introducion}
The Ising model is a stochastic model of ferromagnetism. On the microscopic level we consider a discrete system of interacting particles on the $\Lambda_N = \ZZ^2 / (2N+1)\ZZ^2$ grid for some $N \in \NN$, where in every grid point sits a spin with exactly two possible state: $\pm1$ (intuitively: the spin pointing in the direction of either the north or the south). The set of corresponding spin configurations will be denoted by $\Sigma_N = \left\{-1, 1\right\}^{\Lambda_N}$. In the standard Ising model the particles are interacting only with their immediate neighbours, while in the Ising-Kac model the particles can influence the behaviour of the other particles in a small radius proportional to a scaling factor $\gamma > 0$. To build intuition about the behaviour of the system, we can imagine to have a probability distribution on $\Sigma_N$ depending on the inverse temperature $\beta > 0$, under which configurations with close-by spins aligned in the same direction have in general higher probability, than configurations where the spins are completely disorganised. The system is more likely to go from a less probable configuration to a more probable one, but the reverse transition is also not excluded, especially at low inverse temperatures. The locally averaged field describes the strength of the field at each gridpoint, and we will be interested in the fluctuations of this field after suitable renormalization and continuous extension to $\TT^2$. In particular one can show that this random process describing the fluctuations of the field converges in law to the solution of the $\Phi^4_2$ equation 
\begin{align}
  \label{eq:phi_4_2}
  \begin{split}
  \partial_t X(t, x) &= \Delta X(t, x) - \frac{1}{3}X^3(t, x) + A X(t, x) + \sqrt{2}\xi(t, x)\\
  X(0, \cdot) &= X_0\,    
  \end{split}
\end{align}
as long as the initial configurations of the discrete systems converge to $X_0$ in distribution, as we take $N$ to infinity.

We know, that the solution of this equation is distribution valued, and form the mild formulation of the equation it follows that $X_t \in \mathcal{C}^- \DefiningEquality \Union_{\nu > 0} \mathcal{C}^{-\nu}$, which makes the $X_t^3$ term in the equation ill-defined. In the solution theory of the $\Phi^4_2$ equation this is corrected by renormalizing the equation with a diverging linear term, formally written as 
\begin{align}
  \partial_t X(t, x) = \Delta X(t, x) - \frac{1}{3}\left(X^3(t, x) - 3 \infty \times X(t, x)\right) + A X(t, x) + \sqrt{2}\xi(t, x)\,.
\end{align}
We will see in the following, that the need for renormalization is not simply a mathematical artifact of the solution theory, but consistent with the physical interpretation, in the sense that the renormalization represents \say{a shift of the critical temperature away from its mean field value} \cite{mourrat2015convergencetwodimensionaldynamicisingkac}.

\section{The Ising-Kac model}
In order to precisely describe the interaction, we choose an arbitrary kernel $\mathfrak{K}: \RR^2 \to [0, 1]$ satisfying $\mathfrak{K} \in \mathcal{C}^2$, $\Support{\mathfrak{K}} \subset B^2(0, 3) \DefiningEquality \left\{x \in \RR^2 \colon \abs{x} < 3\right\}$, invariant under rotations and having furthermore $$\int_{\RR^2} \mathfrak{K}(x) dx = 1\,,\quad \int_{\RR^2} \mathfrak{K}(x) \abs{x}^2 dx = 4\,.$$


For a scaling factor $0 < \gamma < \frac{1}{3}$ (on which $N = N(\gamma)$ will depend, as we see later), we discretizes the kernel as 
$$\kappa_\gamma: \Lambda_N \to [0, 1]\,,\quad\kappa_\gamma(k) = \begin{cases}
  \frac{\mathfrak{K}(\gamma k)}{\sum_{l \in \Lambda_N} \mathfrak{K}(\gamma l)}&k \neq 0\\
  0 & k = 0
\end{cases}\,,$$
which acts as a mollifier on the grid to define the locally averaged field for any $\sigma \in \Sigma_N$ configuration through a discrete convolution as \begin{align}
  h_\gamma(\sigma, k) \DefiningEquality (\Convolve{\kappa_\gamma}{\sigma})(k) \DefiningEquality  \sum_{l\in \Lambda_N} \kappa_\gamma(k-l) \sigma(l)\,,
\end{align}
where addition is always to be understood modulo $2N + 1$ on the grid: $k + l \DefiningEquality k + l + n(2N + 1) \in \left\{-N, \ldots, N\right\}$ for appropriate $n \in \ZZ$. \footnote{\textcolor{red}{$\gamma \gg N^{-1}$ to avoid self interactions}}

The locally averaged field $h_\gamma$ describes the strength of the magnetic field for a spin at position $k$, and the total energy of the system is defined through the Hamiltonian \begin{align}
  \mathcal{H}_\gamma(\sigma) \DefiningEquality -\frac{1}{2}\sum_{k\in \Lambda_N} \sigma(k) h_\gamma(k)\,.
\end{align} With the help of the Hamiltonian we can define the Gibbs measure on $\Sigma_N$ for any $\beta > 0$ inverse temperature as \begin{align}
  \lambda_\gamma(\sigma) = \frac{1}{\mathcal{Z}_\gamma}\exp{\left(-\beta \mathcal{H}_\gamma(\sigma)\right)}\,,\quad \mathcal{Z}_\gamma = \sum_{\sigma \in \Sigma_N} \exp{\left(-\beta \mathcal{H}_\gamma(\sigma)\right)}\,.
\end{align}

To describe the evolution of the process we define a Markov process on $\Sigma_N$ with the defined by the generator \begin{align}
  \mathcal{L}_\gamma(f)(k) = \sum_{l \in \Lambda_N} c_\gamma(\sigma, l)(f(\sigma^j) - f(\sigma))
\end{align}
for functions $f: \Lambda_N \to \RR$, where we let $\sigma^j \in \Sigma_N$ denote the configuration identical to $\sigma$, except for the spin at position $j$, which we flipped. For the jump rates $c_\gamma(\sigma, l)$, we choose the those of the Glauber dynamics, which can be simplified as
\begin{align}
  \label{eq:trig_jump_rates}
  c_\gamma(\sigma, l) = \frac{\lambda(\sigma^l)}{\lambda(\sigma) + \lambda(\sigma^l)} = \frac{e^{-\sigma(j)\beta h_\gamma(\sigma, j)}}{e^{\beta h_\gamma(\sigma, j)} + e^{-\beta h_\gamma(\sigma, j)}} = \frac{1}{2}\left(1 - \sigma(j) \tanh\left(\beta h_\gamma(\sigma, j)\right)\right)\,,
\end{align}
where the second equality follows from \begin{align}
  \TODO{explain}\,,
\end{align}
and the second equality follows from
\begin{align}
  \TODO{trig identity}\,.
\end{align}
For the Markov process generated by the semigroup we write $(\sigma(t))_{t \ge 0}$, and for the corresponding process generated by the observable $h_\gamma(\cdot, \cdot)$ we write $(h_\gamma(t, k))_{t \ge 0} \DefiningEquality h_\gamma(\sigma(t), k)$ for each $k \in \Lambda_N$, for which we can write the evolution equation
\begin{align}
  \label{eq:microscopic_evolution_equation}
  h_\gamma(t, k) = h_\gamma(0, k) + \int_{0}^{t} \mathcal{L}_\gamma h(s, k) \,ds + m_\gamma(t, k)\,
\end{align}
where $(m(t, k))_{t\ge 0}$ is a martingale for each $k\in \Lambda_N$\cite[Appendix B]{mourrat2015convergencetwodimensionaldynamicisingkac}, since the $\sigma(\cdot, k)$ process is a Feller process \cite[Section 6.5.1]{gall2016brownian}. Using the trigonometric representation of the jump rates \eqref{eq:microscopic_evolution_equation}, the Taylor expansion $\tanh(h) = h - \frac{h^3}{3} + \smallO{h^6}$ and the fact, that $h_\gamma(\sigma^j, k) - h_\gamma(\sigma, k) = -2 \kappa_\gamma(k-j) \sigma(j)$ we obtain 
\begin{align}
  \mathcal{L}_\gamma h_\gamma(\sigma, k) &= \left(\Convolve{\kappa_\gamma}{h_\gamma(\sigma, k)} - h_\gamma(\sigma, k)\right) + (\beta - 1) \Convolve{\kappa_\gamma}{h_\gamma(\sigma, k)} - \frac{\beta^3}{3}\left(\Convolve{\kappa_\gamma}{h_\gamma^3(\sigma, k)}\right) + \operatorname{err}\\
  &\DefiningEqualityRev \bar{\Delta}_\gamma h_\gamma(\sigma, k) + (\beta - 1) \Convolve{\kappa_\gamma}{h_\gamma(\sigma, k)} - \frac{\beta^3}{3}\left(\Convolve{\kappa_\gamma}{h_\gamma^3(\sigma, k)}\right) + \operatorname{err}\,.
\end{align}
We use the evolution equation \eqref{eq:microscopic_evolution_equation} to derive an SPDE after suitable rescaling on $\TT^2$, identified with $[0, 1]^2$. For this purpose we choose $\gamma \in (0, 1)$, $N = N(\gamma)$ to be defined later, $\epsilon = \frac{2}{2N + 1}$ and $\alpha, \delta > 0$ scaling factors, and we assign to every $k \in \Lambda_N$ the macroscopic coordinates by scaling $$k = (k_1, k_2) \mapsto \epsilon k =(\epsilon k_1, \epsilon k_2) \in \TT^2\,.$$ Let us in particular denote $\Lambda_\epsilon = \epsilon \Lambda_N$, and for these points we consider the rescaled field
\begin{align}
  X_\gamma(t, x) = \frac{1}{\delta}h_\gamma\left(\frac{t}{\alpha}, \frac{x}{\epsilon}\right)~(x\in \Lambda_\epsilon, t \ge 0)\,.
\end{align}
From the evolution equation \eqref{eq:microscopic_evolution_equation} after rescaling we get
\begin{align}
  \label{eq:evolution_in_macroscopic_coordintes}
  \begin{split}
      X_\gamma(t, x) =& X_\gamma(0, x) + \int_{0}^{t} \biggl(\frac{\epsilon^2}{\gamma^2 \alpha}\tilde{\Delta}_\gamma X_\gamma(s, x) + \frac{\beta - 1}{\alpha} \Convolve[\epsilon]{K_\gamma}{X_\gamma(s, x)}\\
    &-\frac{\beta^3}{3} \frac{\delta^2}{\alpha} \Convolve[\epsilon]{K_\gamma}{X_\gamma^3(s, x)} + \Convolve[\epsilon]{K_\gamma}{E_\gamma(s, x)}\biggr)\,ds + M_\gamma(t, x)\,,
  \end{split}
\end{align}
where $K_\gamma(x) = \epsilon^{-2} \kappa_\gamma(\epsilon^{-1}x) = c_\gamma \frac{\gamma^2}{\epsilon^2}\mathfrak{K}\left(\frac{\gamma}{\epsilon}x\right)$ is our rescaled kernel, $M_\gamma(t, x) \DefiningEquality \frac{1}{\delta}m_\gamma\left(\frac{t}{\alpha},\frac{x}{\epsilon}\right)$ is the rescaled martingale, $\tilde{\Delta}_\gamma = \frac{\gamma^2}{\epsilon^2} \bar{\Delta}_\gamma$ is the rescaled discrete Laplacian and $\Convolve[\epsilon]{}{}$ is the discrete convolution on $\Lambda_\epsilon$. The error term $E_\gamma(s, x)$ is furthermore given explicitely as 
\begin{align}
  \label{eq:error_term}
  E_\gamma(t, \cdot) = \frac{1}{\delta \alpha}\left(\tanh\left(\beta \delta X_\gamma(t, \cdot)\right) - \beta \delta X_\gamma(t, \cdot) + \frac{(\beta \delta)^3}{3} X_\gamma(t, \cdot)^3\right)\,.
\end{align}
To fix the above appearing scaling factors as a function of the Kac scaling parameter $\gamma$, we have to observe the following constraints 
\begin{itemize}
  \item $N = \floor{\lambda^{-2}}$,
  \item  $\epsilon = \frac{2}{2N + 1}$ is by definition the distance between adjacent particles in macroscopic coordinates,
  \item the coefficients $\frac{\epsilon^2}{\gamma^2 \alpha}$ and $\frac{\delta^2}{\alpha}$ should converge to $1$ as $\gamma \to 0^+$, for the convergence of the corresponding terms in \eqref{eq:evolution_in_macroscopic_coordintes} to something non-trivial,
\end{itemize}
which results in $\alpha = \gamma^2, \delta=\gamma$. In partcular, the leading order term in the expansion of the error \eqref{eq:error_term} scales as $\frac{(\beta \delta)^5}{\delta\alpha} = \beta^5 \gamma^2$, which we would expect to converge to $0$ for an appropriate choice of $\beta$, whose value was not yet set. Based on \eqref{eq:evolution_in_macroscopic_coordintes} a reasonable choice could be $\frac{\beta - 1}{\alpha} \approx 1$, which turns out to be not the right one. The authors refer to \cite{cassandro1997upper}, in which it was already observed that at least in the stationary case a shift from the 


\TODO{finish this phrase, conclude from (2.18) that it's a logarithmically divergent term}.

\TODO{state the evolution equation with all the terms replaced, also redefined the $\Delta$ operator}

\begin{itemize}
  % \item we rescale time, space and amplitude by factors $\alpha$, $\epsilon$ and $\delta$, respectively, and by considering the evolution equation after the coordinate change we obtain the right factors so that the leading terms have coefficient 1 in the rescaled equation.
  \item this gives a guess for the inverse temperature, but the trivial guess (O(1)) is incorrect as already observed in [10]
  \begin{itemize}
    \item in the same paper they say \say{The origin of the Wick regularization in the limit theory is then explained in terms of the critical temperature shift found in Theorem 1.1, which is in fact recognized as the microscopic origin of the Wick regularization.}
    \item instead we choose something that such that the term $\frac{\beta -1}{\alpha} = \mathfrak{c}_\gamma +A$ where $A$ is fixed and $\mathfrak{c}_\gamma = \Theta(\log{\gamma^{-1}})$. "The extra term $\mathfrak{c}_\gamma$ reflects the fact that the limiting equation has to be renormalized" (p7)
  \end{itemize}
\end{itemize}


$X_\gamma(t, \cdot)$ was so far defined only for the points $x \in \Lambda_\epsilon$, thus if we wanted to talk about it's convergence to $X$, we have to first extend it to the whole $\TT^2$. For the sake of convenience we do it with the help of trigonometric polynomials, since they play well with the tools of Fourier analysis, but the choice is essentially irrelevant \cite{mourrat2015convergencetwodimensionaldynamicisingkac}. Thus for any $Y: \Lambda_\epsilon \to \RR$ we define the discrete Fourier transform $\hat{Y}$ as \begin{align}
  \hat{Y}(k) = \begin{cases}
      \sum_{x \in \Lambda_\epsilon} \epsilon^2 e^{- i \pi k \cdot x} Y(x)&k \in \left\{-N, \ldots, N\right\}^2\\
      0&\text{otherwise}
  \end{cases}\,,
\end{align}
form which we get by inverse Fourier transformation $$\Ext Y(x) \DefiningEquality \frac{1}{4} \sum_{k \in \ZZ^2} \hat{Y}(k) e^{i \pi k \cdot x} = \frac{1}{4} \sum_{k \in \left\{-N, \dots, N\right\}^2} \sum_{y \in \Lambda_\epsilon}\epsilon^2 e^{i \pi k \cdot (x -y)} Y(y)\,.$$
The extension $\Ext Y$ is smooth, real valued, it's the unique trigonometric polynomial of degree $N$ coinciding with $Y$ on $\Lambda_\epsilon$.


\section{Main result}

\begin{theorem}
  Assume that the scaling assumptions hold \TODO{which scaling assumptions?}, where the precise value of $\mathfrak{c}_\gamma$ is given by\begin{align}
    \mathfrak{c}_\gamma = \frac{1}{4}\sum_{\substack{k \in \left\{-N, \ldots, N\right\}^2\\k\neq 0}}\frac{\abs{\hat{K}_\gamma(k)}^2}{\gamma^{-2} (1 - \hat{K}_\gamma(k))}\,.
  \end{align}
  Assume also, that $X^0_\gamma$ converges to $X^0$ in $\mathcal{C}^{-\nu}$ for $\nu > 0$ small enough and that $X^0, x^0_\gamma$ are uniformly bounded in $\mathcal{C}^{-\nu + \kappa}$ for an arbitrarily small $\kappa > 0$. Then $X_\gamma$ converges in law to $X$ with respect to the topology of $\mathcal{D}(\RR_+, \mathcal{C}^{-\nu})$.
\end{theorem}
\TODO{rough sketch of the proof idea}
\chapter{Solution theory of $\Phi^4_2$}
If we considered the mild formulation 
\begin{align}
  X_t = P_t X_0 + \int_{0}^{t}P_{t-s}\left(-\frac{1}{3}X^3(s) + A X(s)\right)\,ds + \int_0^t P_{t-s} \xi(s) \,ds
\end{align}
of the $\Phi^4_2$ equation, then from the $0^-$ regularity of the stochastic convolution $Z(t) = \int_0^t P_{t-s} \xi(s) \,ds$ we cannot expect the solution $X_t$ to have a better regularity then $Z_t$ itself, making the $X^3_s$ distributional product undefined. One could try working around the issue by spatially localizing the stochastic convolution and to take $\epsilon \to 0$ in $Z_\epsilon(t) = \mathcal{F}_{\TT^2}^{-1}(\phi_\epsilon \mathcal{F}_{\TT^2}(t, \Placeholder))$ where $\phi_\epsilon = \CharFunction{B(0, \epsilon^{-1})}$, for example. It has been shown in \cite{hairer2012triviality}, that under such an approximation the solution $X_\epsilon$ converges to the constant $0$ distribution.

In order to make sense of the equation one has to introduce a shift in
\begin{align}
  X_\epsilon(t) = P_t X_0 + \int_{0}^{t}P_{t-s}\left(-\frac{1}{3}\left(X^3_\epsilon(s) - \mathfrak{c}_\epsilon X_\epsilon\right)+ A X(s)\right)\,ds + \int_0^t P_{t-s} \xi(s) \,ds
\end{align}
with $\mathfrak{c}_\epsilon = \sum_{0 < \abs{k} < \epsilon^{-1}} \frac{1}{4 \pi^2 \abs{k}^2}$ in order to reach a non-trivial limit when we take $X_\epsilon$ to the limit. To make this precise, we decompose first $X_t$ as $X_t = Z_t + v_t$, and notice that $v_t$ now solves the 
\begin{align}
  \label{eq:phi_eq_in_v}
  \begin{split}
    \partial_t v &= \Delta v - \frac{1}{3}(v + Z)^3 + (v + Z) = \Delta v - \frac{1}{3}(v^3 + 3v^2 Z + 3v Z^2 + Z^3) + (v+Z)\\
    v_0 &= X^0
  \end{split}
\end{align}
equation. If we assume for the initial datum $X^0 \in \mathcal{C}^\alpha~(\alpha \in (1, 2))$, and if we considered now $(Z, Z^1, Z^2)$ as arbitrary processes (ie, the superscripts denoting indices, and not exponentiation) with $Z^i \in {C}(\RR_+, \mathcal{C}^{\alpha-2})$, then we get from setting up a Picard iteration that there exists a random time $T^\star$ depending on $(Z, Z^2, Z^3)$ and a unique solution $v$ depending continuously on $(X^0, Z, Z^1, Z^2)$ \cite[Theorem 4.10.]{perkowski2024stochaIV}.

Since $Z_\epsilon$ is a smooth function, we can actually construct its powers and apply the above result, then take the $\epsilon \to 0$ limit in the hope of obtaining a solution $v$. Unfortunately $Z_\epsilon^2$ diverges as $\epsilon \to 0$. To correct this, one replaces the $n^\text{th}$ power with the $n^\text{th}$ Hermite polynomial as
\begin{align}
  Z_\epsilon^{:n:}(t, x) \DefiningEquality H_n(Z_\epsilon(t, x), \mathfrak{c}_\epsilon(t))\,
\end{align}
where $H_0(x, t) \equiv 1$, $H_{n}(x, t) = x H_{n-1}(x, t) - t \partial_x H_{n-1}(x, t)~(n \ge 1)$ and
\begin{align*}
  \mathfrak{c}_\epsilon(t) = \Exp{Z_\epsilon(t, 0)^2}\,.
\end{align*}
One can show the existence of the limits in $\epsilon \to 0$:
\begin{theorem}[Proposition 3.1. in \cite{mourrat2015convergencetwodimensionaldynamicisingkac}]
  For every $T > 0$ and every $\nu > 0$, the stochastic processes $Z_\epsilon$ and $Z_\epsilon^{:n:}$ for $n \ge 2$ converge almost surely and in every stochastic $L^p$ space with respect to the metric of $\mathcal{C}([0, T], \mathcal{C}^{-\nu})$. We denote the limiting processes by $Z$ and $Z^{:n:}$.
\end{theorem}
In the above proof it is crucial that we can represent $Z_\epsilon^{:n:}(x, t)$ as an object in the $n^\text{th}$ homogenous Wiener-ItÃ´ chaos, meaning we can write $Z_\epsilon^{:n:}(t, x)$ as an iterated stochastic integral of a symmetric deterministic function against a Gaussian noise. Guassianity of the noise let us bound higher moments of $Z_\epsilon^{:n:}(t, x)$ by $\Exp{Z_\epsilon^{:n:}(t, x)^2}$, which in turn leads to bounds on $\Exp{\norm{Z_\epsilon^{:n:}(t, \Placeholder)}^p_{\mathcal{C}^{-\nu}}}$ uniform in $\epsilon$ to guarantee tightness, and bounds on $\Exp{\norm{Z_\epsilon^{:n:}(t, \Placeholder) - Z_\epsilon^{:n:}(s, \Placeholder)}^p_{\mathcal{C}^{-\nu}}}$ to get continuity in time from Kolmogorov's continuity criteria.

For the convergence of the discretized model, it will be slightly more convenient \cite[p.14]{mourrat2015convergencetwodimensionaldynamicisingkac} to set the initial datum to $v_0 = 0$, thus a shift of the Wick products of the stochastic convolution is necessary. For $Y(t) \DefiningEquality P_t X^0$, let us define
\begin{align}
  \label{eq:z_tilde_redefinition}
  \begin{split}
    \tilde{Z}(t, \Placeholder) &= Y(t, \Placeholder) + Z(t, \Placeholder)\\
    \tilde{Z}^{:2:}(t, \Placeholder) &= Z^{:2:}(t, \Placeholder) + 2Y(t, \Placeholder) Z(t, \Placeholder) + Y(t, \Placeholder)^2\\
    \tilde{Z}^{:3:}(t, \Placeholder) &= Z^{:3:}(t, \Placeholder) + 3 Y(t, \Placeholder) Z(t, \Placeholder)^{:2:} + 3Y(t, \Placeholder)^2 Z(t, \Placeholder) + Y(t, \Placeholder)^3\,.
  \end{split}
\end{align}
Then for $A(t) \DefiningEquality A + \lim_{\epsilon \to 0} (\mathfrak{c}_\epsilon - \mathfrak{c}_\epsilon(t))$, the equation
\begin{align}
  \label{eq:new_eq_in_v}
  \partial_t v &= \Delta v - \frac{1}{3}(v^3 + 3 \tilde{Z}v^2 + 3 \tilde{Z}^2 v + \tilde{Z}^3) + A(t) (\tilde{Z} + v)\\
  v(0, \Placeholder) &= 0
\end{align}
has a unique solution up until some random time $T^\star$, and the solution is \say{stable under approximation of the functions $Z_\epsilon$, $Z_\epsilon^{:2:}$ and $Z_\epsilon^{:3:}$ in $\mathcal{C}([0, \infty), \mathcal{C}^{-\nu}$} \cite{mourrat2015convergencetwodimensionaldynamicisingkac}. From \cite{mourrat2017global} we have moreover for a fixed initial datum and any $T>0$
\begin{theorem}
  For $\nu > 0$ small enough, fix an initial datum $X^0 \in \mathcal{C}^{-\nu}$. For $(Z, Z^{:2}, Z^{:3:}) \in (L^\infty([0, T], \mathcal{C}^{-\nu}))^3$, let $(\tilde{Z}, {\tilde{Z}^{:2:}}, \tilde{Z}^{:3:})$ be defined as in \eqref{eq:z_tilde_redefinition}. Let $\mathcal{S}_T(Z, Z^{:2:}, Z^{:3:})$ denote the solution $v$ on $[0, T]$ of the PDE \eqref{eq:new_eq_in_v}. Then for any $\kappa > 0$, the mapping $\mathcal{S}_T$ is Lipschitz continuous on bounded sets from $(L^\infty([0, T], \mathcal{C}^{-\nu}))^3$ to $\mathcal{C}([0, T], \mathcal{C}^{2 - \nu -\kappa}(\TT^2))$.
\end{theorem}

% \begin{itemize}
%   \item hontestly, I should maybe just handwave it, and state proposition 3.2, saying that a continuous solution operator exists.
%   % \item linearized solution $\tilde{X}$ lies in $\mathcal{C}^-$, so no way to interpret $\tilde{X}^3$ consistently, and no hope that the solution of the non-linear equation is any more regular
%   % \begin{itemize}
%   %   \item Perkowski had a slightly different argument, claiming that the solution is not better than the stochastic convolution, but I think it's pretty much the same
%   %   \item on the other hand if we spatially regularized the white noise by $$W_\epsilon(t, x) = \frac{1}{4}\sum_{\lvert k \rvert < \epsilon^{-1}} e^{i \pi k \cdot x}\hat{W}(k, t) = \mathcal{F}^{-1}_{\mathbb{T}^2}(\varphi_\epsilon \mathcal{F}_{\mathbb{T}^2}W(t, \cdot))(x)$$then the solution $X_\epsilon$ of $$dX_\epsilon = (\delta X_\epsilon - \frac{1}{3} X^3_\epsilon + A X_\epsilon) dt + \sqrt{2} dW_\epsilon$$converges in probability to $0$
%   %   % \item for the record, $Z_\epsilon(t, \cdot) = \int_0^t P_{t-s} dW_\epsilon(s, \cdot)$ is the same as localizing the heat kernel and convolving it with $dW$, which is again the same as spatially localizing the stochastic convolution (think about the Fourier cutoff projector as a self-adjoint linear operator, thus we can pull it in, by considering how the stochastic convolution acts on test functions)
%   %   % \item in order to obtain a nontrivial limit, we have to adjust the above approximation by adding a linear term with logarithmically diverging coefficient $\mathfrak{c}_\epsilon = \sum_{0 < |k| < \epsilon^{-1}}\frac{1}{4\pi^2 | k|^2}$.
%   % \end{itemize}
%   % \item Consider the mild solution $Z_\epsilon(t, \cdot) = \int_0^t P_{t-s} dW_\epsilon(s, \cdot)$ of the linearized equation. We can define $Z_\epsilon^n$ but this won't converge for $n \ge 2$. Instead of that we consider $Z^{:n:}_\epsilon(t, \cdot) = H_n(Z(t, \cdot), \mathfrak{c}_\epsilon)$ which will converge as $\epsilon \to 0$ with 
% \end{itemize}

% \begin{itemize}
  % \item variant of the ferromagnetic Ising model
  % \begin{itemize}
  %   \item Ising-Kac: not only closest neighborhood interactions, but in a small radius of $\gamma^{-1}$
  %   \item using Glauber dynamics for the jump probabilities
  %   \item "random fluctuations of a \[...\] magnetisation field $X_\gamma$ in the limit $\gamma \to 0$, for inverse temperature close to the critical temperature of the mean field model" 
  %   \item ~"under suitable assumptions [...] these fields converge in law to the solution of the $\Phi^4_2$ equation"
  % \end{itemize}
  % \item $\Phi^4_2$ is ill defined, solution theory is more "intricate" (p1) (than that of $\Phi^4_1$)
  % \begin{itemize}
  %   \item solution is at best as good as the stochastic convolution term, no "consistent" definition of the term $X^3$,
  %   \item renormalization needed, so instead of $$\partial_t X(t, x) = \Delta X(t, x) - \frac{1}{3} X^3(t, x) + A X(t, x) + \sqrt{2}\xi(t, x)$$we formally solve $$\partial_t X(t, x) = \Delta X(t, x) - \frac{1}{3} (X^3(t, x) - 3 \infty \times X(t, x))+ A X(t, x) + \sqrt{2}\xi(t, x)$$(for the solution theory see Perkowski, or directly da Prato-Debussche)
  %   \item other solution theory, which allows to solve much more singular SPDEs, motivation: "to show that fluctuations of non-linear particle systems are governed by such an equation". 
  % \end{itemize}
  % \item interesting feature of the result: "it gives a natural interpretation for the infinite renormalisation constant as a shift of critical temperature away from its mean field value"
  % \item use of the Kac-type interactions prevents the model from converging to the continuous Ising model
% \end{itemize}
\chapter{Setting and main result}
\begin{itemize}
  % \item microscopic system
  % \begin{itemize}
  %   \item the setting on $\Lambda_N = \mathbb{Z}^2 / (2N + 1)\mathbb{Z}^2$ is quite clear
  %   % \begin{itemize}
  %   %   \item except some constraints on the kernel:
  %   %   \begin{itemize}
  %   %     \item supported on a ball of radius 3 (I guess to make it interesting enough?)
  %   %     \item why do we have $\mathfrak{K}(0) = 0$  (to avoid self interactions? if we didn't have this, then the mean field would be a constant higher or lower, no?)
  %   %   \end{itemize}
  %   % \end{itemize}
  %   \item locally averaged field $\to$ Hamiltonian $\to$ Gibbs measure $\to$ Glauber dynamics $\to$ $(\sigma_t)_{t\ge 0}$ pure jump Markov process on $\Sigma_N$ describing the evolution of the spin configurations, and the corresponding $h_\gamma(\sigma)$ local mean field
  %   % \item (Le Gall): $h_\gamma(t, k) = h_\gamma(0, k) + \int_0^t \mathcal{L} h_\gamma(s, k)\,ds + m_\gamma(t, k)$, where $(m_\gamma(t, k))_{t \ge 0}$ is a martingale for all $k \in \Lambda_N$. The [[predictable quadratic covariation]] of the martingale are given in $(2.10)$.
  %   % \begin{itemize}
  %   %   \item note: if we discretized the time and we interpreted $\mathcal{L}$ as $P-I$ operator, then it's clear why $m$ is a martingale.
  %   % \end{itemize}
  % \end{itemize}
  % \item macroscopic coordinates:
  % \end{itemize}
  % \item extension of functions given on the macroscopic coordinates to the whole $\mathbb{T}^2$:
  % \begin{itemize}
  %   % \item discrete Fourier transform on discrete coordinates, then inverse Fourier transform on the whole $\mathbb{T}^2$ $\textcolor{red}{\text{Q: why do we get something real valued here}}$   
  %   % \begin{itemize}
  %   %   \item the extension will coincide with the original function on the discrete points
  %   %   \item extension is compatible with known results: well behavedness with respect to convolution and Parseval.
  %   % \end{itemize}
  % \end{itemize}
  \item main result:
  \begin{itemize}
    \item $\mathcal{D}(\mathbb{R}_+, \mathcal{S})$: $\mathcal{S}$ metric space-valued cÃ dlÃ g functions on $\mathbb{R}_+$ endowed with the Skorokhod topology
    \item $\mathcal{C}^{-\nu}~(\nu > 0)$ Besov space, with the def Perkowski used!
    \item just state Theorem 2.1
  \end{itemize}
\end{itemize}

\chapter{Bounds for the linearized system}
\begin{itemize}
  \item we consider the mild solution of the linearized evolution $$\begin{aligned}dZ_\gamma(t, x) &= \Delta_\gamma Z_\gamma(t, x) dt + dM_\gamma(t, x)\\Z_\gamma(0, x) &= 0 \end{aligned}$$which is $Z_\gamma(t, x) = \int_{r=0}^t P^\gamma_{t-s} dM_\gamma(s, x)$, where $P^\gamma_t = e^{\Delta_\gamma t}$ is the semigroup generated by $\Delta_\gamma X = \frac{\gamma^2}{\epsilon^2}(K_\gamma \star_\epsilon X - X)$. 
  \item we define $R_{\gamma, t}(s, x) := \int_{r=0}^s P^\gamma_{t-r} dM_\gamma(r, x)~(x \in \Lambda_\epsilon)$  which we can extend to trigonometric polynomials the usual way. This is an integral against a [[cÃ dlÃ g]] martingale, thus $R_{\gamma ,t}$ is also a [[cÃ dlÃ g]] martingale for all $x \in \mathbb{T}^2$
  \begin{itemize}
    \item we recursively define $R_{\gamma, t}^{:1:} := R_{\gamma, t}$ and $R_{\gamma, t}^{:n:}(s,x) := n\int_{r=0}^s R_{\gamma, t}^{:n-1:}(r^-,x) \,dR_{\gamma, t}(r, x)$, where $r^-$ is the left limit (to make the integrand predictable, which is necessary for integration against cadlag martingales (to make the integral a martingale again))
    \begin{itemize}
      \item in order to extend $R_{\gamma, t}^{:n:}$ to $\mathbb{T}^2$, we don't just use trig polys, but we consider the Fourier series $$\hat{R}_{\gamma, t}^{:n:}(s,x) = \frac{1}{4}n\int_{r=0}^s R_{\gamma, t}^{:n-1:}(r^-,x) \,dR_{\gamma, t}(r, x)$$and let $R_{\gamma, t}^{:n:}(s, x) = \frac{1}{4} \sum_{k \in \mathbb{Z}^2} \hat{R}^{:n:}_{\gamma, t}(s, k)e^{i \pi k \cdot x}$.
    \end{itemize}
    \item we can now define $Z^{:n:}_\gamma(t, x) = R^{:n:}_{\gamma, t}(t, x)$
  \end{itemize}
  \item the goal of this section to obtain bounds on the above quantities. The difficulty is that now we don't have access to Gaussian noise, so no gaussian hypercontractivity (Thm 4.21 in Perkowski) (I guess this is what called Nelson's estimate in this paper). One can replace this with BDG inequality + controlling the jumps (p12, par(-1)).
  \begin{itemize}
    \item Lemma 4.1: $$\mathbb{E}\left[\sup_{0\le r \le t} |\mathfrak{I}_n F(r)|^p\right]^{p/2} < \infty$$and actually it gives a much more precise bound, but maybe it'd be more interesting to state this bound for the exact $F$ we are applying it, and $\mathfrak{I}_n$ is just the $n^\text{th}$ iterated Wiener-ItÃ´ integral.
    \item Proposition 4.2: from the previous lemma we can obtain for some $\gamma_0 > 0$, that for every $n\in \mathbb{N}, p \ge 1, \nu > 0, T> 0, 0 \le \lambda \le 1/2, 0 < \kappa \le 1$ there is a constant $C = C(n, p, \nu, T, \lambda, \kappa)$ such that for every $0 \le s \le t \le T$ and $0 < \gamma < \gamma_0$:
    \begin{itemize}
      \item $\lVert Z^{:n:}_\gamma(t, \cdot) \rVert^p_{\mathcal{C}^{-\nu - 2\lambda}} \le C t^{\gamma p} + C \gamma^{p(1-\kappa)}$ 
      \item $\lVert Z^{:n:}_\gamma(t, \cdot) - Z^{:n:}_\gamma(s, \cdot) \rVert^p_{\mathcal{C}^{-\nu - 2\lambda}} \le C \lvert t- s\rvert^{\gamma p} + C \gamma^{p(1-\kappa)}$ 
      \item $\lVert Z^{:n:}_\gamma(t, \cdot) - R^{:n:}_{\gamma, t}(s, \cdot) \rVert^p_{\mathcal{C}^{-\nu - 2\lambda}} \le C \lvert t- s\rvert^{\gamma p} + C \gamma^{p(1-\kappa)}$ (these are some weaker bound coming from the main claim, but we'll use it in this form. it's unclear so far how the previous lemma was used in this theorem)
    \end{itemize}
    \item Lemmas 4.4, 4.5 are for proposition 4.2
    \item Lemma 4.6 seems to be interesting and important: it reflects the fact that Proposition 4.2 is a bit too rough as an estimate, and we can give a much better estimate on the high frequencies of $Z_\gamma$ defined as $Z_\gamma^{\text{high}}(t, x)$:
  \end{itemize}
\end{itemize}
\chapter{Tightness for the linearized system}
\begin{itemize}
  \item Discussion of the terms $Z_\gamma^{:n:}$ and $R_{\gamma, t}^{:n:}$ is continued. After having obtained bounds on the goodness of the approximation in Prop 4.2, we give additional bounds related to the [[predictable quadratic variation]]s (for $M$ [[cÃ dlÃ g]] $L^2$ martingale it's the unique $\langle M \rangle$ making $M^2 -  \langle M \rangle$ into a martingale) and the [[bracket process]] $[M]_t = M^2_t - 2\int_0^t M(s^-)dM(s)$, namely
  \begin{itemize}
    \item a bound on their differences
    \item a bound on the $L^p$ norm of the $\sup_{x \in \Lambda_\epsilon} [R_{\gamma, t}(\cdot, x)]$ 
  \end{itemize}
  \item we define now $E^{:n:}_{\gamma, t}(s, x) = H_n(R_{\gamma, t}(s, x), [R_{\gamma, t}(\cdot, x)]_s) - R^{:n:}_{\gamma, t}(s, x)~(x\in \mathbb{T}^2)$ where $[R_{\gamma, t}(\cdot, x)]_s$ is the trig extension of $[R_{\gamma, t}(\cdot, x)]_s$. The claim of Proposition 5.3 is that we can approximate $R^{:n:}_{\gamma, t}(s, x)$ as a Hermite polynomial with the following error bound:$$\mathbb{E}[\sup_{x\in\mathbb{T}^2}\sup_{0\le s \le t} \lvert E^{:n:}_{\gamma, t}(s, x)\rvert^p]^{1/p} \le C \gamma^{1 - \kappa}$$
  \begin{itemize}
    \item does this imply that for some $x$ the minimum error is not necessarily obtained for $s = t$?
    \item for $x\in \Lambda_{\epsilon}$ is the error $0$? I guess not, since then from Lemma A.6 the whole error would be 0
  \end{itemize}
  \item this allows us to conclude the tightness of the processes $Z^{:n:}_\gamma$ on $\mathcal{D}(\mathbb{R}_+, \mathcal{C}^{-\nu})$, which according to Billingsley implies relative compactness. In order to prove tightness, it's enough to prove that the family is tight when restricted to every compact time interval. The statement precisely:
  \begin{itemize}
    \item For any fixed $n\in \mathbb{N}$ and any $\nu > 0$, the family $\left\{Z^{:n:}_{\gamma}, \gamma \in (0, 1/3) \right\}$ is tight on $\mathcal{D}(\mathbb{R}_+, \mathcal{C}^{-\nu})$. Any weak limit is supported on $\mathcal{C}(\mathbb{R}_+, \mathcal{C}^{-\nu})$. Furthermore, for any $p \ge 1$ and $T > 0$, we have $$\sup_{\gamma \in (0, 1/3)} \mathbb{E}\left[ \sup_{0 \le t \le T} \lVert Z^{:n:}_\gamma(t, \cdot)\rVert^p_{\mathcal{C}^{-\nu}} \right] < \infty$$
    \item some interesting points of the proof:
    \begin{itemize}
      \item WLOG $\gamma \in (0, \gamma_0)$, we fix $T$, and we prove tightness on $\mathcal{D}([0, T], \mathcal{C}^{-\nu})$
      \item let $m \in \mathbb{N}$ be fixed later, $p \ge 1, \nu^\prime > 0, \lambda \le (2m)^{-1}$ and $n \in \mathbb{N}$. Then from Proposition 4.2. (in particular claim (4.19) and its consequence Remark 4.3) we have $$\mathbb{E}\left[ \lVert Z_\gamma^{:n:}(t, \cdot) - Z_\gamma^{:n:}(s, \cdot)\rVert^p_{\mathcal{C}^{-\nu^\prime -2\lambda}} \right]\le C\lvert t-s \rvert^{\lambda p}$$
      \begin{itemize}
        \item on the other hand: we discretize time: consider $\gamma^m \mathbb{N}_0$  times and we approximate linearly $Z_\gamma^{:n:}$ on this grid. for the linear approximation $\tilde{Z}_\gamma^{:n:}$ we apparently also have $$\mathbb{E}\left[ \lVert \tilde{Z}_\gamma^{:n:}(t, \cdot) - \tilde{Z}_\gamma^{:n:}(s, \cdot)\rVert^p_{\mathcal{C}^{-\nu^\prime -2\lambda}} \right]\le C\lvert t-s \rvert^{\lambda p}$$from the construction of $\tilde{Z}_\gamma^{:n:}$, so we can apply the sufficient Kolmogorov-Chentsov criterion for tightness (Corollary 14.9 form Kallenberg), from which the desired properties follow for $\tilde{Z}^{:n:}_\gamma$
      \end{itemize}
      \item now we set up a bound $$\mathbb{E}\left[ \sup_{0 \le t \le T} \sup_{x \in \mathbb{T}^2} \lvert \tilde{Z}^{:n:}_\gamma(t, x) - {Z}^{:n:}_\gamma(t, x) \rvert^p \right]\le C(n, p, T, \kappa) \gamma^{(1-\kappa)p}$$from which we can transfer the properties form $\tilde{Z}^{:n:}_\gamma$ to ${Z}^{:n:}_\gamma$.
    \end{itemize}
  \end{itemize}
\end{itemize}
\chapter{Convergence in law of the linearised system}
\begin{itemize}
  \item goal is to show that $Z_\gamma$ converges to $Z$ in law, and so do $Z_\gamma^{:n:}$ to $Z^{:n:}$. Problem: we can only do this up to a stopping time which depends on $X_\gamma$, the non-linear dynamics, about which we know yet nothing. For any $\nu \in (0, 1/2), \mathfrak{m} > 1$ and $0 < \gamma < 1$, let$$\tau_{\gamma, \mathfrak{m}} = \inf \left\{ t \ge 0 : \lVert X_\gamma(t, \cdot) \rVert_{\mathcal{C}^{-\nu}} \ge \mathfrak{m}\right\}$$and we set up $\sigma_{\gamma, \mathfrak{m}}(t, k)$ which is the same as $\sigma(t, k)$ up until $\tau_{\gamma, \mathfrak{m}}$, and essentially random afterwards ("jumps occurring for every $t > \tau_{\gamma, \mathfrak{m}}/\alpha$ and every $k\in \Lambda_N$ at rate $1/2$, independently from $\sigma$"). We construct the rest the same, $\sigma$ replaced by $\sigma_{\gamma, \mathfrak{m}}$, and all the properties hold, since the only property of jump rates were that they are $\le 1$, and this still holds.
  \item Main theorems of this section:
  \begin{enumerate}
    \item Let $\nu \in (0, 1/2)$ and $\mathfrak{m} > 1$. As $\gamma$ tends to $0$, the process $Z_{\gamma, \mathfrak{m}}$ converges in law to $Z$ with respect to the Skorokhod topology on $\mathcal{D}(\mathbb{R}_+, \mathcal{C}^{-\nu})$, where $Z$ is defined in Proposition 3.1 ("the stochastic convolution"), where the main idea of the proof is
    \begin{itemize}
      \item Since all the previous properties of the previously constructed objects still remains true for the newly constructed process, so in particular we have tightness. Consider any subsequential limit $\bar{Z}$. From the martingale characterization of the solution of the stochastic heat equation we can see that $Z = \bar{Z}$.
    \end{itemize}
    \item (Convergence of $Z_\gamma^{:n:}$). For every $\mathfrak{m} \in \mathbb{N}$ and $n \in \mathbb{N}$, the processes $(Z_{\gamma, \mathfrak{m}}^{:1:}, \ldots, Z_{\gamma, \mathfrak{m}}^{:n:})$ converge (jointly) in law to $(Z^{:1:}, \ldots, Z^{:n:})$ with respect to the topology of $\mathcal{D}(\mathbb{R}_+, \mathcal{C}^{-\nu})^n$.
    \begin{itemize}
      \item the proof idea is to show it through the convergence of finite dimensional distributions (look up the exact result in Billingsley)
      \item Lemma A.6 is used again for convergence on non-grid points (but maybe in an argument that I won't even present)
    \end{itemize}
  \end{enumerate}
  \item Questions
  \begin{itemize}
    \item So does this converge in law to $Z$, because for any $\mathfrak{m} >1$ we have that $\tau_{\gamma, \mathfrak{m}} \to \infty$ as $\gamma \to 0$?
    \item Do we already have that $X_{\gamma, \mathfrak{m}}$ converges to $X$ in law?
  \end{itemize}
\end{itemize}
\chapter{Analysis of the non-linear equation}
We consider $\nu > 0$ small enough and fixed, and some initial data $X^0 \in \mathcal{C}^{-\nu}$ also fixed throughout the section. We denote $X \in \mathcal{C}(\mathbb{R}_+, \mathcal{C})$ the solution of the renormalized $\Phi^4_2$ equation as constructed in Section 3.

We denoted with $X_\gamma$ the rescaled field and it satisfied the evolution equation on the points of $\Lambda_\epsilon$. The problem is that it's not true anymore for $\mathbb{T}^2 \setminus \Lambda_\epsilon$ (since Ext does not commute with cubing), so we have to derive a new equation.
\begin{itemize}
  \item this is done in Lemma 7.1, with a new error term $\operatorname{Err}^{(1)}(s, \cdot)$, which is bounded by something for which it's not immediately clear that converges to $0$ as $\gamma \to 0$ (at least to me ), and in particular that bound is random (because)
  \item After this technical lemma, we construct a new process:
  \begin{itemize}
    \item in the evolution equation we replace $Z_\gamma$ with $Z_{\gamma, \mathfrak{m}}$ and $\operatorname{Err}^{(1)}$ with $\operatorname{Err}^{(1)}_\mathfrak{m}(s) = \begin{cases}\operatorname{Err}^{(1)}(s)&s \le \tau_{\gamma, \mathfrak{m}}\\ 0 &\text{otherwise}\end{cases}$  and consider a solution $X_{\gamma, \mathfrak{m}}$, which  exists from an elementary ODE argument 
    \begin{itemize}
      \item $\textcolor{red}{\text{fixpoint argument? does the solution exists for the whole } \mathbb{R}_+ \text{ almost surely?}}$  
    \end{itemize}
    \item in particular the $X_{\gamma, \mathfrak{m}} = X_\gamma$ on the event $\left\{ t < \tau_{\gamma, \mathfrak{m}}\right\}$ 
    \begin{itemize}
      \item $\textcolor{red}{\text{why not with }\le\text{?}}$ 
      \item using this fact we can apply the previous error bound to $\operatorname{Err}^{(1)}_\mathfrak{m}$  up to $\tau_{\gamma, \mathfrak{m}}$, after which the error is simply $0$.
    \end{itemize}
  \end{itemize}
  \item We construct another approximation of $X_\gamma$. For $0 \le t \le T$ we define:$$\overline{X}_{\gamma, \mathfrak{m}}(t, \cdot) = P_t X^0 + Z_{\gamma, \mathfrak{m}}(t, \cdot) + \mathcal{S}_T(Z_{\gamma, \mathfrak{m}}, Z^{:2:}_{\gamma, \mathfrak{m}}, Z_{\gamma, \mathfrak{m}}^{:3:})(t, \cdot)$$where $\mathcal{S}_T$ is the solution operator for some fixed $X^0$ initial datum. 
  \begin{itemize}
    \item so here $\overline{X}_{\gamma, \mathfrak{m}}$ is the solution of the following SPDE: $\textcolor{red}{\text{figure this out!}}$ 
    \item in Lemma 7.2 we have: $\overline{X}_{\gamma, \mathfrak{m}}$ converges to $X$ with respect to $\mathcal{D}([0, T], \mathcal{C}^{-\nu})$, where in particular we use the uniform bounds bounds from Proposition 5.4, so that the paths of $Z_{\gamma, \mathfrak{m}}^{:i:}$ are almost surely bounded. But in the end this is just a use of the continuous mapping theorem
  \end{itemize}
  \item Now we want to bound the difference between $\overline{X}_{\gamma, \mathfrak{m}}$ (the result got from the solution operator $\mathcal{S}_T$) and $X_\gamma$ (the process from the evolution equation, we can also eventually consider $X_{\gamma, \mathfrak{m}}$, if it's more advantageous). The main goal of the remaining is to obtain the following bound for every fixed $\mathfrak{m} \ge 1, T > 0$ and bounded uniformly continuous function $F: \mathcal{D}([0, T], \mathcal{C}^{-\nu}) \to \mathbb{R}$: $$\limsup_{\gamma \to 0} \mathbb{E}\left[\lvert F(\overline{X}_{\gamma, \mathfrak{m}}) - F({X}_{\gamma, \mathfrak{m}})\rvert\right] = 0\,.$$Combined with Lemma 7.2 we immediately get the convergence of ${X}_{\gamma, \mathfrak{m}}$ to $X$ in law for every fixed $\mathfrak{m} \ge 1$ (p49). This is proven in Lemmas 7.3, 7.4, 7.5
\end{itemize}
\chapter{Appendices}
\begin{itemize}
  \item Theorem C1: characterization of the solution of the stochastic heat equation through the uniqueness of the martingale problem (it's used in section 6)
  \item Mention at A.6 at a convenient point, it seems to be an important thing, especially 
\end{itemize}


\chapter{Remarks}
\section{Important notes from Billingsley}
\begin{itemize}
  \item If $\Pi$ family of probability measures is tight, then it's relatively compact.
\end{itemize}
\section{Points to observe}
\begin{itemize}
  \item for parameter $\kappa$ we only require arbitrary $> 0$, but form the type of estimates we get actually only $\kappa \in (0, 1)$ would make sense.
  \item we often restrict ourselves to $\gamma \in (0, \gamma_0)$, where $\gamma_0$ is coming from Lemma 8.2 and Proposition 4.2
\end{itemize}

% \medskip
\newpage
% Bibliography can go on the same page with the appendix.
\printbibliography
\end{document}